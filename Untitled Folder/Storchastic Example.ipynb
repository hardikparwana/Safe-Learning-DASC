{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1114a488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from storch.method import Reparameterization, ScoreFunction\n",
    "import storch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def compute_f(method):\n",
    "    a = torch.tensor(5.0, requires_grad=True)\n",
    "    b = torch.tensor(-3.0, requires_grad=True)\n",
    "    c = torch.tensor(0.23, requires_grad=True)\n",
    "    d = a + b\n",
    "\n",
    "    # Sample e from a normal distribution using reparameterization\n",
    "    normal_distribution = Normal(b + c, 1)\n",
    "    e = method(normal_distribution)\n",
    "\n",
    "    f = d * e * e\n",
    "    return f, c\n",
    "\n",
    "def estimate_variance(method):\n",
    "    gradient_samples = []\n",
    "    for i in range(1000):\n",
    "        f, c = compute_f(method)\n",
    "        storch.add_cost(f, \"f\")\n",
    "        storch.backward()\n",
    "        gradient_samples.append(c.grad)\n",
    "    gradients = storch.gather_samples(gradient_samples, \"gradients\")\n",
    "    # print(gradients)\n",
    "    print(\"variance\", storch.variance(gradients, \"gradients\"))\n",
    "    print(\"mean\", storch.reduce_plates(gradients, \"gradients\"))\n",
    "    print(\"st dev\", torch.sqrt(storch.variance(gradients, \"gradients\")))\n",
    "\n",
    "    print(type(gradients))\n",
    "    print(gradients.shape)\n",
    "    print(gradients.plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24212d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.3038, grad_fn=<SumBackward0>)\n",
      "first derivative estimate tensor(-13.6540)\n",
      "tensor(30.5677, grad_fn=<SumBackward0>)\n",
      "second derivative estimate tensor(-15.6378)\n"
     ]
    }
   ],
   "source": [
    "# based on gaussian process, ghet stochastic estimate of gradient\n",
    "\n",
    "\n",
    "b + c\n",
    "# e*e follows a noncentral chi-squared distribution https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution\n",
    "# exp_f = d * (1 + mu * mu)\n",
    "repar = Reparameterization(\"e\", n_samples=1)\n",
    "f, c = compute_f(repar)\n",
    "storch.add_cost(f, \"f\")\n",
    "print(storch.backward())\n",
    "\n",
    "print(\"first derivative estimate\", c.grad)\n",
    "\n",
    "f, c = compute_f(repar)\n",
    "storch.add_cost(f, \"f\")\n",
    "print(storch.backward())\n",
    "\n",
    "print(\"second derivative estimate\", c.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4e2972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reparameterization n=1\n",
      "variance Deterministic tensor(16.6772) Batch links: []\n",
      "mean Deterministic tensor(-11.0179) Batch links: []\n",
      "st dev Deterministic tensor(4.0838) Batch links: []\n",
      "<class 'storch.tensor.IndependentTensor'>\n",
      "torch.Size([1000])\n",
      "[('gradients', 1000, tensor(0.0010))]\n",
      "Reparameterization n=10\n",
      "variance Deterministic tensor(1.5587) Batch links: []\n",
      "mean Deterministic tensor(-11.0687) Batch links: []\n",
      "st dev Deterministic tensor(1.2485) Batch links: []\n",
      "<class 'storch.tensor.IndependentTensor'>\n",
      "torch.Size([1000])\n",
      "[('gradients', 1000, tensor(0.0010))]\n",
      "Score function n=1\n",
      "variance Deterministic tensor(2989.9502) Batch links: []\n",
      "mean Deterministic tensor(-22.6999) Batch links: []\n",
      "st dev Deterministic tensor(54.6804) Batch links: []\n",
      "<class 'storch.tensor.IndependentTensor'>\n",
      "torch.Size([1000])\n",
      "[('gradients', 1000, tensor(0.0010))]\n",
      "Score function n=45\n",
      "variance Deterministic tensor(66.4302) Batch links: []\n",
      "mean Deterministic tensor(-22.1383) Batch links: []\n",
      "st dev Deterministic tensor(8.1505) Batch links: []\n",
      "<class 'storch.tensor.IndependentTensor'>\n",
      "torch.Size([1000])\n",
      "[('gradients', 1000, tensor(0.0010))]\n",
      "Score function with baseline n=20\n",
      "variance Deterministic tensor(2094.9529) Batch links: []\n",
      "mean Deterministic tensor(-23.2412) Batch links: []\n",
      "st dev Deterministic tensor(45.7707) Batch links: []\n",
      "<class 'storch.tensor.IndependentTensor'>\n",
      "torch.Size([1000])\n",
      "[('gradients', 1000, tensor(0.0010))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reparameterization n=1\")\n",
    "estimate_variance(Reparameterization(\"e\", n_samples=1))\n",
    "\n",
    "print(\"Reparameterization n=10\")\n",
    "estimate_variance(Reparameterization(\"e\", n_samples=10))\n",
    "\n",
    "print(\"Score function n=1\")\n",
    "estimate_variance(ScoreFunction(\"e\", n_samples=1))\n",
    "\n",
    "print(\"Score function n=45\")\n",
    "estimate_variance(ScoreFunction(\"e\", n_samples=45))\n",
    "\n",
    "print(\"Score function with baseline n=20\")\n",
    "estimate_variance(ScoreFunction(\"e\", n_samples=20, baseline_factory=\"batch_average\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e98eff7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'plate_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58149/422721568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscreteVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_58149/422721568.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDiscreteVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScoreFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch_average\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Research/Safe-Learning-DASC/venv_sl_dasc_38/lib/python3.8/site-packages/storch/method/method.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plate_name, sampling_method, n_samples, baseline_factory, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msampling_method\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0msampling_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonteCarlo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplate_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplate_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_factory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaselineFactory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Research/Safe-Learning-DASC/venv_sl_dasc_38/lib/python3.8/site-packages/storch/method/method.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plate_name, sampling_method)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplate_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampling_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplate_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             raise ValueError(\n\u001b[1;32m     45\u001b[0m                 \u001b[0;34m\"The plate name of the sampling method and the storch method should match.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'plate_name'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import storch\n",
    "from storch.method import ScoreFunction\n",
    "\n",
    "class DiscreteVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.method = ScoreFunction(\"z\", 8, baseline_factory=\"batch_average\")\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 20 * 10)\n",
    "        self.fc4 = nn.Linear(20 * 10, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.fc1(x).relu()\n",
    "        h2 = self.fc2(h1).relu()\n",
    "        return self.fc3(h2).reshape(logits.shape[:-1] + (20, 10))\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = z.reshape(z.shape[:-2] + (20 * 10,))\n",
    "        h3 = self.fc4(z).relu()\n",
    "        h4 = self.fc5(h3).relu()\n",
    "        return self.fc6(h4).sigmoid()\n",
    "\n",
    "    def KLD(self, q):\n",
    "        p = torch.distributions.OneHotCategorical(probs=torch.ones_like(q.logits) / (1.0 / 10.0))\n",
    "        return torch.distributions.kl_divergence(p, q).sum(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = torch.distributions.OneHotCategorical(logits=self.encode(x))\n",
    "        KLD = self.KLD(q)\n",
    "        z = self.method(\"z\", q, n=8)\n",
    "        return self.decode(z), KLD\n",
    "\n",
    "model = DiscreteVAE()\n",
    "for data in minibatches():\n",
    "    optimizer.zero_grad()\n",
    "    # Denote the minibatch dimension as being independent\n",
    "    data = storch.denote_independent(data.view(-1, 784), 0, \"data\")\n",
    "\n",
    "    # Compute the output of the model\n",
    "    recon_batch, KLD = model(data)\n",
    "\n",
    "    # Register the two cost functions\n",
    "    storch.add_cost(KLD)\n",
    "    storch.add_cost(storch.nn.b_binary_cross_entropy(recon_batch, data, reduction=\"sum\"))\n",
    "\n",
    "    # Go backward through both deterministic and stochastic nodes\n",
    "    average_ELBO, _ = storch.backward()\n",
    "    print(average_ELBO)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cc4ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58149/453066239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvae\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScoreFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch_average\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vae'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import storch\n",
    "from vae import minibatches, encode, decode, KLD\n",
    "\n",
    "method = storch.method.ScoreFunction(\"z\", 8, baseline_factory=\"batch_average\")\n",
    "for data in minibatches():\n",
    "    optimizer.zero_grad()\n",
    "    # Denote the minibatch dimension as being independent\n",
    "    data = storch.denote_independent(data.view(-1, 784), 0, \"data\")\n",
    "\n",
    "    # Define the variational distribution given the data, and sample latent variables\n",
    "    q = torch.distributions.OneHotCategorical(logits=encode(data))\n",
    "    z = method(q)\n",
    "\n",
    "    # Compute and register the KL divergence and reconstruction losses to form the ELBO\n",
    "    reconstruction = decode(z)\n",
    "    storch.add_cost(KLD(q))\n",
    "    storch.add_cost(storch.nn.b_binary_cross_entropy(reconstruction, data, reduction=\"sum\"))\n",
    "\n",
    "    # Go backward through both deterministic and stochastic nodes, and optimize\n",
    "    average_ELBO, _ = storch.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d317e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement vae (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for vae\u001b[0m\r\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/hardik/Desktop/Research/Safe-Learning-DASC/venv_sl_dasc_38/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b640306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/hardik/Desktop/Research/Safe-Learning-DASC/venv_sl_dasc_38/lib/python3.8/site-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Using cached pip-21.2.4-py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "Successfully installed pip-21.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd34bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
