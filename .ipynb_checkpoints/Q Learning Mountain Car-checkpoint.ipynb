{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Q Learning Mountain Car\n",
    "Discret version of mountain-car problem using Q-learning (offline/off-policy) and SARSA (online/on-policy).\n",
    "\n",
    "![poster.jpg](data:image/jpeg;base64,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)\n",
    "\n",
    "#### **Mountain Car Problem**####\n",
    "\n",
    "1. State: a two dimensional vector \n",
    "- Car position: [-1.2,0.6]\n",
    "- Car velocity: [-0.07,0.07]\n",
    "\n",
    "  If we discretize the state space into 20x20 cells. There are 400 states in total. \n",
    "2. Actions:\n",
    "- 0 : Accelerate to the left\n",
    "- 1 : no acceleration \n",
    "- 2 : Accelerate to the right\n",
    "3. Reward:\n",
    "\n",
    "- Reward of 0 is given if the agent reaches the flag (position >= 0.5) on top of the mountain.\n",
    "- Reward of -1 is given if the position of the agent is lower than 0.5.\n",
    "\n",
    "4. Initial position:\n",
    "  The initial position of the car is a value chosen uniformly at random from [-0.6, -0.4].\n",
    "\n",
    "5. An episode Terminates when one of the following occurs:\n",
    "- The car position is higher than 0.5\n",
    "- The episode length is greater than 200\n",
    "\n",
    "\n",
    "**Programming Language**\n",
    "\n",
    "Python 3.6x with following libraries only:\n",
    "- Numpy\n",
    "- Pandas\n",
    "- Scikit-Learn\n",
    "- gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some functions for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym\n",
    "import copy\n",
    "from gym import Env\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "Discrete the Environment\n",
    "'''\n",
    "\n",
    "class DiscretizeEnv(Env):\n",
    "    def __init__(self,env,num_bins,show_info=False):\n",
    "      self.env = env\n",
    "      self.num_bins = num_bins\n",
    "      self.action_space = self.env.action_space\n",
    "      self.observation_space = self.env.observation_space\n",
    "      self.low = self.env.observation_space.low\n",
    "      self.high = self.env.observation_space.high\n",
    "      self._max_episode_steps = env._max_episode_steps \n",
    "      self.discrete_states = [\n",
    "            np.linspace(self.low[0], self.high[0], num=(num_bins + 1))[1:-1],\n",
    "            np.linspace(self.low[1], self.high[1], num=(num_bins + 1))[1:-1],\n",
    "        ]\n",
    "      self.action_dim = 3\n",
    "      self.state_dim = self.num_bins ** len(self.discrete_states)\n",
    "      if show_info:\n",
    "          print(f'State space: Position Range:({self.low[0]:.2f},{self.high[0]:.2f}) Velocity Range:({self.low[1]:.2f},{self.high[1]:.2f})')\n",
    "          print(f'State space shape: {env.observation_space.shape}')\n",
    "          print(f'Action space shape: {env.action_space}')\n",
    "          print(f'Dimension of state after discreting: {self.state_dim}')\n",
    "    \n",
    "    def get_index(self,state):\n",
    "      #print(state)\n",
    "      state_ind = sum(np.digitize(feature, self.discrete_states[i]) * (self.num_bins ** i)\n",
    "                    for i, feature in enumerate(state))\n",
    "      return state_ind\n",
    "\n",
    "    def reset(self):\n",
    "      state = self.env.reset()\n",
    "      #print(f\"reset state: {state}\")\n",
    "      return self.get_index(state)\n",
    "\n",
    "    def render(self):\n",
    "      self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "      next_state,reward,done,_ = self.env.step(action)\n",
    "      if next_state[0] >= 0.5:\n",
    "          reward = 0 \n",
    "      next_state_ind = self.get_index(next_state)\n",
    "      return next_state_ind,reward,done\n",
    "\n",
    "'''Evaluate with an given policy'''\n",
    "\n",
    "def eval_policy(agent,bins,eval_episodes=10):\n",
    "    env_name = 'MountainCar-v0'\n",
    "    eval_env = gym.make(env_name)\n",
    "    eval_env = DiscretizeEnv(eval_env,bins)\n",
    "    avg_reward = 0.\n",
    "    for _ in range(eval_episodes):\n",
    "        state, done = eval_env.reset(), False\n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            state, reward, done = eval_env.step(action)\n",
    "            avg_reward += reward\n",
    "    avg_reward /= eval_episodes\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    return avg_reward\n",
    "\n",
    "'''Return an episode with a given policy'''\n",
    "\n",
    "def return_episode(agent,bins):\n",
    "    env_name = 'MountainCar-v0'\n",
    "    gen_env = gym.make(env_name)\n",
    "    #gen_env._max_episode_steps = 2000\n",
    "    gen_env = DiscretizeEnv(gen_env,bins)\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    done_list = []\n",
    "    \n",
    "    state, done = gen_env.reset(), False\n",
    "    state_list.append(state)\n",
    "    episode_timesteps = 0\n",
    "    while not done:\n",
    "        episode_timesteps += 1\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, done = gen_env.step(action)\n",
    "        done_bool = float(done) if episode_timesteps < gen_env._max_episode_steps else 0\n",
    "        action_list.append(action)\n",
    "        state_list.append(state)\n",
    "        reward_list.append(reward)\n",
    "        if done:\n",
    "            if not done_bool:\n",
    "                done_list.append(1)\n",
    "            else:\n",
    "                done_list.append(-1)\n",
    "        else:\n",
    "            done_list.append(0)\n",
    "    return state_list,action_list,reward_list,done_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Position Range:(-1.20,0.60) Velocity Range:(-0.07,0.07)\n",
      "State space shape: (2,)\n",
      "Action space shape: Discrete(3)\n",
      "Dimension of state after discreting: 400\n"
     ]
    }
   ],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "bins = 20  #Discrete the state space into 20 x 20.\n",
    "env = DiscretizeEnv(env,bins,show_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful predefined functions. Assume we already have a discreted environment like above.\n",
    "1. ```env.reset()```\n",
    "\n",
    "  Reset the environment. You can get the initial state by calling ```state=env.reset()```.\n",
    "2. ```next_state, reward, done = env.step(action)```\n",
    "\n",
    "  Interacting with the environment and get the **next_state**, **reward**, **system info** by taking an action. The system info *done* here indicates either reach the maximize length or get to the goal. (-1 terminal(reach the goal); 1: termianl(max length) 0: otherwise)\n",
    "3. ```avg_reward = eval_policy(agent,bins=20,eval_episodes=10)```\n",
    "\n",
    "  Evaluate a given policy (agent) for 10 episodes and return avg_reward.\n",
    "4. ```state_list,action_list,reward_list,done_list = return_episode(agent,bins=20)```\n",
    "\n",
    "  Return an episode with a given policy (agent). Index starts from 0. The length of state_list is one step longer than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Position Range:(-1.20,0.60) Velocity Range:(-0.07,0.07)\n",
      "State space shape: (2,)\n",
      "Action space shape: Discrete(3)\n",
      "Dimension of state after discreting: 400\n",
      "random agent\n",
      "episode:0,reward:-200.0\n",
      "episode:1,reward:-200.0\n",
      "episode:2,reward:-200.0\n"
     ]
    }
   ],
   "source": [
    "class Random_Agent(object):\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        print(\"random agent\")\n",
    "    def select_action(self,state):\n",
    "        #TODO:  This function takes a state as input and outputs an action (0,1,2 for mountain car problem). \n",
    "        return int(np.random.choice(3, 1))\n",
    "    \n",
    "    \n",
    "env_name = 'MountainCar-v0'\n",
    "env_example = gym.make(env_name)\n",
    "env_example = DiscretizeEnv(env_example,num_bins=20,show_info=True)\n",
    "agent = Random_Agent()\n",
    "    \n",
    "for ii in range(3):\n",
    "    state = env_example.reset()\n",
    "    ep_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    while True: \n",
    "        #env_example.render() Uncomment this line to see the real interaction between agent and the environment. But you can only see it locally.\n",
    "        episode_timesteps += 1\n",
    "        action = agent.select_action(state)\n",
    "        next_state,reward,done  = env_example.step(action)\n",
    "        next_action = agent.select_action(next_state)     \n",
    "        done_bool = float(done) if episode_timesteps < env_example._max_episode_steps else 0 #Check whether it is end because of reaching the maximize length or getting to the goal\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            print(f'episode:{ii},reward:{ep_reward}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both of these two classes have two inputs, which are dtate_dim(dimension of state) and action_dim (dimension of action).\n",
    "\n",
    "BETA = 0.3\n",
    "BETA_MIN = 0.00005\n",
    "GAMMA = 0.98\n",
    "EPSILON = 0.9\n",
    "EPSILON_DECAY = 0.95\n",
    "EPISODES_MAX = 100000\n",
    "EPISODES = 100000\n",
    "\n",
    "class Q_learning(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = 0.6                        ## control random choice 0.6\n",
    "        self.epsilon_decay = 0.9995 \n",
    "        self.counter = 0\n",
    "        self.gamma = 0.99                         \n",
    "        self.beta = 0.4                           ## learning rate 0.4 also works\n",
    "        self.beta_decay = 0.9995\n",
    "        self.beta_min = 0.00005\n",
    "        self.Q_table = np.zeros([20*20,3])\n",
    "        \n",
    "\n",
    "    def train(self,state_list,action_list,reward_list,done_list):\n",
    "        '''\n",
    "        done_list[i](ith element): -1 terminal(reach the goal); 1: termianl(max length) 0: otherwise\n",
    "        TODO: \n",
    "        state_list: states for one episode\n",
    "        action_list: actions for one episode\n",
    "        reward_list: rewards for one episode\n",
    "        done_list: system info, and also done_list[i](ith element): -1 terminal(reach the goal); 1: termianl(max length); 0: otherwise\n",
    "        '''\n",
    "\n",
    "        for index in range(np.size(state_list)-1):\n",
    "          # if (reward_list[index]==0):\n",
    "          #   print(\"found\")\n",
    "          if not done_list[index]:\n",
    "            Q_next = np.max(np.array(self.Q_table[state_list[index+1]]))\n",
    "            self.Q_table[state_list[index]][action_list[index]] += self.beta*( reward_list[index] + self.gamma*Q_next - self.Q_table[state_list[index]][action_list[index]] )\n",
    "          else:\n",
    "            self.Q_table[state_list[index]][action_list[index]] += self.beta*(reward_list[index] - self.Q_table[state_list[index]][action_list[index]] )\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter>=1:\n",
    "          self.epsilon *= self.epsilon_decay\n",
    "          if self.beta > (self.beta_min/self.beta_decay):\n",
    "            self.beta *= self.beta_decay\n",
    "          self.counter = 0\n",
    "                    \n",
    "\n",
    "    def select_action(self,state):\n",
    "        if (np.random.random()<self.epsilon):\n",
    "            action = int(np.random.choice(3, 1))\n",
    "        else:\n",
    "            action = np.argmax(np.array(self.Q_table[state]))\n",
    "\n",
    "        return action\n",
    "\n",
    "class SARSA(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.epsilon = 0.4\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.counter = 0\n",
    "        self.gamma = 0.98\n",
    "        self.beta = 0.3\n",
    "        self.beta_decay = 0.9995\n",
    "        self.beta_min = 0.00005\n",
    "        self.Q_table = np.zeros([20*20,3])\n",
    "        \n",
    "\n",
    "    def train(self,s0,a0,r0,s1,a1,sign):\n",
    "        '''\n",
    "        sign: -1 terminal(reach the goal); 1: termianl(max length) 0: otherwise\n",
    "        TODO: \n",
    "        s0 (current state), a0 (current action), r0 (reward0, s1 (next_state), a1 (next action)\n",
    "        '''\n",
    "        if (sign==0):\n",
    "          Q_next = self.Q_table[s1][a1]\n",
    "          self.Q_table[s0][a0] += self.beta*( r0 + self.gamma*Q_next - self.Q_table[s0][a0] )\n",
    "        else:                              #if going to next episode now: then decrement beta dn epsilon\n",
    "          self.Q_table[s0][a0] += self.beta*( r0 - self.Q_table[s0][a0] )                 \n",
    "          self.epsilon *= self.epsilon_decay\n",
    "          if self.beta > (self.beta_min/self.beta_decay):\n",
    "            self.beta *= self.beta_decay\n",
    "\n",
    "    def select_action(self,state):\n",
    "\n",
    "        if (np.random.random()<self.epsilon):\n",
    "            action = int(np.random.choice(3, 1))\n",
    "        else:\n",
    "            action = np.argmax(np.array(self.Q_table[state]))\n",
    "        \n",
    "        return action\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###   unique max!! take\n",
    "def return_episode_for_Q_learning(agent, env_):\n",
    "    #gen_env._max_episode_steps = 2000\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    reward_list = []\n",
    "    done_list = []\n",
    "    \n",
    "    state, done = env_.reset(), False\n",
    "    state_list.append(state)\n",
    "    episode_timesteps = 0\n",
    "    while not done:\n",
    "        episode_timesteps += 1\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, done = env_.step(action)\n",
    "        done_bool = float(done) if episode_timesteps < env_._max_episode_steps else 0\n",
    "        action_list.append(action)\n",
    "        state_list.append(state)\n",
    "        reward_list.append(reward)\n",
    "        if done:\n",
    "            if not done_bool:\n",
    "                done_list.append(1)\n",
    "            else:\n",
    "                done_list.append(-1)\n",
    "        else:\n",
    "            done_list.append(0)\n",
    "\n",
    "    return state_list,action_list,reward_list,done_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Position Range:(-1.20,0.60) Velocity Range:(-0.07,0.07)\n",
      "State space shape: (2,)\n",
      "Action space shape: Discrete(3)\n",
      "Dimension of state after discreting: 400\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -200.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -200.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -194.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -175.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -138.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -137.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -125.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -136.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -131.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -151.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -133.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -155.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -126.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -120.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -135.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -132.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -140.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -121.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -150.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -141.900\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -151.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -150.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -134.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -148.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -138.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -140.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -143.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -166.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -151.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -159.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -145.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -177.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -168.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -161.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -138.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.400\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -158.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -156.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -145.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -124.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -150.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -139.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -129.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -141.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -146.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -159.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -140.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -130.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -135.100\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.400\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -153.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -145.800\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -148.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -142.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -169.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -152.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -144.200\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -140.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -180.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -148.300\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -135.700\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -147.600\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -151.000\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -157.500\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "Evaluation over 10 episodes: -173.800\n",
      "---------------------------------------\n",
      "----------- seconds ------------\n",
      "1089.1065735816956\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train Q Learning\n",
    "env_name = 'MountainCar-v0'\n",
    "env_ = gym.make(env_name)\n",
    "env_ = DiscretizeEnv(env_,num_bins=20,show_info=True)\n",
    "env_._max_episode_steps = 2000\n",
    "agent = Q_learning(2,3)\n",
    "\n",
    "reward_Q = []\n",
    "Q_nonzero = []\n",
    "\n",
    "# For Q learning\n",
    "for ii in range(EPISODES):\n",
    "    #print(ii)\n",
    "    #while True: \n",
    "    #env_example.render() Uncomment this line to see the real interaction between agent and the environment. But you can only see it locally.\n",
    "    state_list,action_list,reward_list,done_list = return_episode_for_Q_learning(agent,env_)\n",
    "    #print(\"episode generated\")\n",
    "    agent.train(state_list,action_list,reward_list,done_list)\n",
    "    #print(\"trained\")\n",
    "    # print(agent.Q_table)\n",
    "    if ii%1000==0:\n",
    "      test_reward = eval_policy(agent,20,10)\n",
    "      #print(agent.epsilon)\n",
    "      #print(agent.beta)\n",
    "      #print(len(np.nonzero(agent.Q_table)[0]))\n",
    "      reward_Q.append(test_reward)\n",
    "      Q_nonzero.append( len(np.nonzero(agent.Q_table)[0]) )  \n",
    "      if test_reward>=-100:\n",
    "        break\n",
    "      if (time.time() - start_time)>40*60:\n",
    "        break;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"----------- seconds ------------\")\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Position Range:(-1.20,0.60) Velocity Range:(-0.07,0.07)\n",
      "State space shape: (2,)\n",
      "Action space shape: Discrete(3)\n",
      "Dimension of state after discreting: 400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e8b42f63451c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#env_example.render() Uncomment this line to see the real interaction between agent and the environment. But you can only see it locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mepisode_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0msign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a25a319aaa85>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mnext_state_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a25a319aaa85>\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m#print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       state_ind = sum(np.digitize(feature, self.discrete_states[i]) * (self.num_bins ** i)\n\u001b[0m\u001b[1;32m     38\u001b[0m                     for i, feature in enumerate(state))\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-a25a319aaa85>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0;31m#print(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       state_ind = sum(np.digitize(feature, self.discrete_states[i]) * (self.num_bins ** i)\n\u001b[0m\u001b[1;32m     38\u001b[0m                     for i, feature in enumerate(state))\n\u001b[1;32m     39\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train SARSA\n",
    "env_name = 'MountainCar-v0'\n",
    "env_ = gym.make(env_name)\n",
    "env_ = DiscretizeEnv(env_,num_bins=20,show_info=True)\n",
    "env_._max_episode_steps = 2000\n",
    "agent = SARSA(2,3)\n",
    "agent.epsilon = EPSILON\n",
    "\n",
    "# For Q learning\n",
    "for ii in range(EPISODES):\n",
    "    s0, done = env_.reset(), False\n",
    "    a0 = agent.select_action(s0)\n",
    "    episode_timesteps = 0\n",
    "    if ii%1000==0:\n",
    "        agent.epsilon = agent.epsilon*EPSILON_DECAY\n",
    "    if agent.epsilon<0.01:\n",
    "        print(f'episode:{ii},epsilon:{agent.epsilon}')\n",
    "    while not done: \n",
    "        #env_example.render() Uncomment this line to see the real interaction between agent and the environment. But you can only see it locally.\n",
    "        episode_timesteps += 1        \n",
    "        s1, r0, done = env_.step(a0)\n",
    "        a1 = agent.select_action(s1)\n",
    "        sign = 0\n",
    "        if done==1:\n",
    "            sign = -1\n",
    "        elif episode_timesteps >= env_._max_episode_steps:\n",
    "            done = 1\n",
    "            sign = 1\n",
    "        agent.train(s0,a0,r0,s1,a1,sign)\n",
    "        a0 = a1\n",
    "        s0 = s1\n",
    "        \n",
    "        \n",
    "eval_policy(agent,20,10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
